---
title: Using a local LLM with Breadboard via LM Studio
aliases: []
tags:
  - breadboard/phase/2
  - lm_studio
  - local_ai
created: 2024-05-31T19:25:49
modified: 2024-06-13T11:13:20
---

<iframe width="560" height="315" src="https://www.youtube.com/embed/0qr_Tk39zWg?rel=0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

## Introduction

Here we demonstrates the integration of a local language model with a Breadboard using the LM Studio software. Below, we will guide you through the process and explain the functionality of each input parameter.

## Objective

The goal of this project is to showcase how easy it is to integrate a breadboard with a language model and to provide a step-by-step example using the LM Studio software.

## Prerequisites

- LM Studio software installed on your local machine.
- A basic understanding of language models and breadboards.

## Inputs and Parameters

### 1. Max Tokens

- **Description**: Specifies the maximum length of the response generated by the language model.
- **Example Value**: `-1` (indicating no limit on response length for this example).

### 2. Stream

- **Description**: A flag that indicates whether the language model should return the response as it is being generated.
- **Example Value**: `true` or `false`.

### 3. System Context

- **Description**: Defines the context in which the language model should reply.
- **Example Value**: `"Professional Chef"` (in this example, the language model responds as a professional chef).

### 4. Temperature

- **Description**: Adjusts the creativity of the language model's responses. Lower values make the model more deterministic.
- **Example Value**: `1` (to ensure a balanced response).

### 5. User Context

- **Description**: The question or command provided to the language model.
- **Example Value**: `"List the ingredients needed to make a cake."`

## Step-by-Step Guide

### Step 1: Setting Up Inputs

1. **Max Tokens**: Set to `-1` for this example.
2. **Stream**: Set to `true` or `false` based on your requirement.
3. **System Context**: Set to `"Professional Chef"`.
4. **Temperature**: Set to `1` to maintain response consistency.
5. **User Context**: Enter the question `"List the ingredients needed to make a cake."`

### Step 2: Running the Board

- Execute the board with the provided inputs.
- Open LM Studio to observe the response generation in real time.

### Step 3: Viewing the Response

- The response will be generated word by word if the stream flag is set.
- Observe the final response in LM Studio or on your board.

## Example Output

### Request

```markdown
Max Tokens: -1
Stream: true
System Context: "Professional Chef"
Temperature: 1
User Context: "List the ingredients needed to make a cake."
```

### Response

```markdown
To make a cake, you will need the following ingredients:

- Flour
- Sugar
- Eggs
- Butter
- Baking powder
- Milk
- Vanilla extract
```

## Conclusion

This example illustrates the simplicity and potential of integrating breadboards with language models using LM Studio. By following the steps outlined above, you can create responsive and contextually aware applications.

We hope you found this guide useful. For further inquiries or advanced configurations, feel free to contact our support team.
